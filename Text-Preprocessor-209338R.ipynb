{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5615 - Information Retrieval - Assignment 1 - Basic Text Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author - KATS JAYATHILAKA (209338R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = open(\"assignment_data.txt\")\n",
    "raw_data = datafile.read()\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting all letters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_data = raw_data.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nonum_data = re.sub(r'\\d+', '', lc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "nopunc_data = nonum_data.translate(str.maketrans(\"\",\"\", string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing trailing and ending whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_data = nopunc_data.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Before executing the tokenization code in the next cell, go to the python shell in your conda evironment and type the following commands. This needs to be done only once.\n",
    "\n",
    "`import nltk\n",
    "nltk.download()`\n",
    "\n",
    "Then an installation window appears. Go to the 'Models' tab and select 'punkt' from under the 'Identifier' column. Then click Download and it will install the necessary files. Then no errors would occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens: 1215\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens = nltk.word_tokenize(stripped_data)\n",
    "print(f\"No. of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'O',\n",
       " '’',\n",
       " 'Neill',\n",
       " 'thinks',\n",
       " 'that',\n",
       " 'the',\n",
       " 'boys',\n",
       " '’',\n",
       " 'stories',\n",
       " 'about',\n",
       " 'Chile',\n",
       " '’',\n",
       " 's',\n",
       " 'capital',\n",
       " 'aren',\n",
       " '’',\n",
       " 't',\n",
       " 'amusing']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str = 'Mr. O’Neill thinks that the boys’ stories about Chile’s capital aren’t amusing'\n",
    "nltk.word_tokenize(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multi-task']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"multi-task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clear any previous output\n",
    "open(\"tokenizer_output.txt\", \"w\").close()\n",
    "\n",
    "tof = open(\"tokenizer_output.txt\", \"a\")\n",
    "for token in tokens:\n",
    "    tof.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Correction\n",
    "This steps takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spc = SpellChecker()\n",
    "spell_corrected_tokens = [spc.correction(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spell Correction accuracy check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absence', 'absentee', 'absence', 'allegiance', 'allegiance', 'allegiance', 'apparent', 'apparent', 'apparent', 'conscious', 'conscious', 'committed', 'committed', 'definitely', 'definately', 'defiantly', 'hygiene', 'hygiene', 'hygiene', 'hygiene', 'hygiene', 'mischievous', 'mischievous', 'mischievous', 'succesful', 'successful', 'successful', 'tommorrow', 'tommorrow', 'vacuum', 'vacuum', 'value', 'writing', 'writing', 'welfare', 'welfare']\n"
     ]
    }
   ],
   "source": [
    "# ISOLATED SPELL CHECK\n",
    "incorrect_words = [\n",
    "    'absense', 'absentse', 'abcense',\n",
    "    'allegaince', 'allegience', 'alegiance',\n",
    "    'apparant', 'aparent', 'apparrent',\n",
    "    'concious', 'consious',\n",
    "    'commited', 'comitted',\n",
    "    'definitly', 'definately', 'defiantly',\n",
    "    'hygene', 'hygine', 'hiygeine', 'higeine', 'hygeine',\n",
    "    'mischievious', 'mischevous', 'mischevious',\n",
    "    'succesful', 'successfull', 'sucessful',\n",
    "    'tommorow', 'tommorrow',\n",
    "    'vaccuum', 'vaccum', 'vacume',\n",
    "    'writting', 'writeing',\n",
    "    'wellfare', 'welfair'\n",
    "]\n",
    "print([spc.correction(iw) for iw in incorrect_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The demonstrtors gahered near the SpaceX faciity, where the company builds its Falcon 9 rockets, lining up along Crenshaw Boulevard, CBS LA repoted.\n",
      "Ther aim: to stop the rocket company from launchin Tukey's Turksat 5A communication saellite.\n",
      "SpaceX is epected to launch the mission Nov. 30 from Cape Canaveral, Florida using a Falcon 9, acording to Spaceflight Now.\n",
      "(Some press and social medi reports have referred to the satellite as Turksat 1A, but it is Turksat 5A tha SpaceX will launch.)\n"
     ]
    }
   ],
   "source": [
    "# CONTEXT SENSITIVE SPELL CHECK\n",
    "sentences = []\n",
    "sentences.append(\"The demonstrtors gahered near the SpaceX faciity, where the company builds its Falcon 9 rockets, lining up along Crenshaw Boulevard, CBS LA repoted.\")\n",
    "sentences.append(\"Ther aim: to stop the rocket company from launchin Tukey's Turksat 5A communication saellite.\")\n",
    "sentences.append(\"SpaceX is epected to launch the mission Nov. 30 from Cape Canaveral, Florida using a Falcon 9, acording to Spaceflight Now.\")\n",
    "sentences.append(\"(Some press and social medi reports have referred to the satellite as Turksat 1A, but it is Turksat 5A tha SpaceX will launch.)\")\n",
    "\n",
    "for s in sentences:\n",
    "    print(spc.correction(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "The standard stop words are taken from the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of stop words in the package: 318\n",
      "No. of tokens after removing stop words: 742\n",
      "No. of stop words removed from original tokens: 473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(f\"No. of stop words in the package: {len(ENGLISH_STOP_WORDS)}\")\n",
    "\n",
    "results = [t for t in tokens if not t in ENGLISH_STOP_WORDS]\n",
    "print(f\"No. of tokens after removing stop words: {len(results)}\")\n",
    "\n",
    "print(f\"No. of stop words removed from original tokens: {len(tokens)-len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(word) for word in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Before executing the lemmatization code in the next cell, go to the python shell in your conda evironment and type the following commands. This needs to be done only once.\n",
    "\n",
    "`import nltk\n",
    "nltk.download('wordnet')`\n",
    "\n",
    "This will download the corpora to your home directory in linux (Downloads in Windows, hopefully) and that default folder will automatically be looked up during lemmatization. The output of the above command execution is as follows.\n",
    "\n",
    "`[nltk_data] Downloading package wordnet to\n",
    "[nltk_data]     /home/singhabahu/nltk_data...\n",
    "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
    "True\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(stem) for stem in stems]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment on Stemming vs Lemmatization\n",
    "Here, the spell-corrected, stop-words-removed list of tokens in the variable <ins>**results**</ins>, are used for both stemming and lemmatization. Only a subset of the tokens are represented for convenience of presentation. Subset size is specified by <ins>**TOKEN_COUNT**</ins>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens\n",
      "['contain', 'good', 'onthisday', '��questions', 'task', 'approach', 'madame', 'influence', 'canada']\n",
      "After Stemming (No lemmatization done)\n",
      "['contain', 'good', 'onthisday', '��question', 'task', 'approach', 'madam', 'influenc', 'canada']\n",
      "After Lemmatization (No stemming done)\n",
      "['contain', 'good', 'onthisday', '��questions', 'task', 'approach', 'madame', 'influence', 'canada']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "TOKEN_COUNT = 10\n",
    "\n",
    "random_results = []\n",
    "\n",
    "for i in range(0, (TOKEN_COUNT-1)):\n",
    "    random_results.append(results[random.randint(0, len(results))])\n",
    "\n",
    "print(\"Original Tokens\")\n",
    "print(random_results)\n",
    "\n",
    "print(\"After Stemming (No lemmatization done)\")\n",
    "print([stemmer.stem(result) for result in random_results])\n",
    "\n",
    "print(\"After Lemmatization (No stemming done)\")\n",
    "print([lemmatizer.lemmatize(result) for result in random_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38test",
   "language": "python",
   "name": "python38test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
